{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "281e86bd6cf548f199027d28cca52134",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/443 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huawei\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\huawei\\.cache\\huggingface\\hub\\models--intfloat--multilingual-e5-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bb6d856a99c49dea997a006dcffb7a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b30c7b49d6c04b9e8217658dca8a9495",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a278ad68d124cc78378573268ff2a53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/167 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1d139cdd12f46aa8f6f022cdd002a9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/655 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b84f5fc2197444e98ccc401b0f57748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/471M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from deep_translator import GoogleTranslator\n",
    "from razdel import sentenize\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import Tensor\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('intfloat/multilingual-e5-small')\n",
    "model = AutoModel.from_pretrained('intfloat/multilingual-e5-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_pool(last_hidden_states: Tensor,\n",
    "                 attention_mask: Tensor) -> Tensor:\n",
    "    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores_calc(input_texts, len_messages):\n",
    "    batch_dict = tokenizer(input_texts, max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    outputs = model(**batch_dict)\n",
    "    embeddings = average_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n",
    "\n",
    "    # normalize embeddings\n",
    "    embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "    scores = (embeddings[:len_messages] @ embeddings[len_messages:].T) * 100\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "scores_calc() missing 1 required positional argument: 'len_messages'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-a9d4ffcbd091>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0minput_texts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mpassage_1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtickers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores_calc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_texts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: scores_calc() missing 1 required positional argument: 'len_messages'"
     ]
    }
   ],
   "source": [
    "passage_1 = \"\"\"üè¶ –°–±–µ—Ä\n",
    "\n",
    "–ö–∞–∫–∏–µ —É –º–µ–Ω—è –º—ã—Å–ª–∏ –ø–æ –∫–æ–º–ø–∞–Ω–∏–∏.\n",
    "\n",
    "–≠—Ç–∞ –∫–æ–º–ø–∞–Ω–∏—è —Å–∞–º–∞—è –¥–æ—Ä–æ–≥–∞—è –≤ –†–æ—Å—Å–∏–∏, –µ–µ —Å—Ç–æ–∏–º–æ—Å—Ç—å –±–æ–ª–µ–µ 6 —Ç—Ä–ª–Ω —Ä—É–±. –ö–æ–Ω–µ—á–Ω–æ –∂–µ –¥–ª—è —ç—Ç–æ–≥–æ –µ—Å—Ç—å –æ—Å–Ω–æ–≤–∞–Ω–∏–µ ‚Äî –æ–Ω–∞ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–∞–º—É—é –±–æ–ª—å—à—É—é –ø—Ä–∏–±—ã–ª—å, –ø–æ –∫—Ä–∞–π–Ω–µ–π –º–µ—Ä–µ –≤ —ç—Ç–æ–º –≥–æ–¥—É –ø—Ä–∏–±—ã–ª—å –º–æ–∂–µ—Ç —Å–æ—Å—Ç–∞–≤–∏—Ç—å –±–æ–ª–µ–µ 1,5 —Ç—Ä–ª–Ω —Ä—É–±., –ø–æ—ç—Ç–æ–º—É –°–±–µ—Ä –ø–æ 6 —Ç—Ä–ª–Ω —É–∂–µ –∏ –Ω–µ –≤—ã–≥–ª—è–¥–∏—Ç —Ç–∞–∫ –¥–æ—Ä–æ–≥–æ. \n",
    "\n",
    "–ö–æ–º–ø–∞–Ω–∏—è –ø–ª–∞—Ç–∏—Ç –¥–æ—Å—Ç–æ–π–Ω—ã–µ –¥–∏–≤–∏–¥–µ–Ω–¥—ã, –∞ —Ç–∞–∫ –∫–∞–∫ –Ω–∞ —Ä—ã–Ω–∫–µ —Å–µ–π—á–∞—Å \"–ø—Ä–∞–≤—è—Ç\" —Ñ–∏–∑–∏–∫–∏, –¥–∏–≤–∏–¥–µ–Ω–¥—ã –≤ —Ç–∞–∫–∏—Ö –∫–æ–º–ø–∞–Ω–∏—è—Ö –±—É–¥–µ—Ç –æ—Å–Ω–æ–≤–æ–ø–æ–ª–∞–≥–∞—é—â–∏–º –≤ —Ä—ã–Ω–æ—á–Ω–æ–π –æ—Ü–µ–Ω–∫–µ. –ù–∞ –¥–∞–Ω–Ω—ã–π –º–æ–º–µ–Ω—Ç —è –æ–∂–∏–¥–∞—é, —á—Ç–æ –°–±–µ—Ä –∑–∞–ø–ª–∞—Ç–∏—Ç –±–æ–ª–µ–µ 33 —Ä—É–±. –¥–∏–≤–∏–¥–µ–Ω–¥–∞–º–∏, –ø—Ä–∏–º–µ—Ä–Ω–æ 34-35 —Ä—É–±. –°–±–µ—Ä –±—É–¥–µ—Ç –ø–ª–∞—Ç–∏—Ç—å –æ–∫–æ–ª–æ 12-14% –¥–∏–≤–∏–¥–µ–Ω–¥–∞–º–∏, —Å–µ–π—á–∞—Å —ç—Ç–æ –Ω–µ –∫–∞–∂–µ—Ç—Å—è –∫–∞–∫–æ–π-—Ç–æ –±–æ–ª—å—à–æ–π –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å—é –ø—Ä–∏ —Å—Ç–∞–≤–∫–µ 16% –∏ –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏ –±–µ–∑—Ä–∏—Å–∫–∞ –≤ –û–§–ó-–ü–ö –æ–∫–æ–ª–æ 16%, –∏ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã—Ö –û–§–ó-–ü–î –æ–∫–æ–ª–æ 12%, –Ω–æ –∫–æ–≥–¥–∞ —Å—Ç–∞–≤–∫—É —Å–Ω–∏–∑—è—Ç –¥–æ 8-9%, —Ç–∞–∫–∞—è –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å –±—É–¥–µ—Ç –≤—ã–≥–ª—è–¥–µ—Ç—å –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω–æ. –¢–µ–º –Ω–µ –º–µ–Ω–µ–µ –Ω–∞–¥–æ –ø–æ–Ω–∏–º–∞—Ç—å, —á—Ç–æ –∏ –û–§–ó-–ü–î –ø—Ä–∏ —Å–Ω–∏–∂–µ–Ω–∏–∏ —Å—Ç–∞–≤–∫–∏ –º–æ–≥—É—Ç –≤—ã—Ä–∞—Å—Ç–∏ –∏ —Å–æ—Å—Ç–∞–≤–∏—Ç—å –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏—é –°–±–µ—Ä—É –∫–∞–∫ –º–∏–Ω–∏–º—É–º –≤ 2024-2025 –≥–≥. \n",
    "\n",
    "–ö–æ–º–ø–∞–Ω–∏—é –æ–¥–Ω–æ–∑–Ω–∞—á–Ω–æ —Å—Ç–æ–∏—Ç —Ä–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å –ø–æ–¥—Ä–æ–±–Ω–æ –≤ 2024 –≥–æ–¥—É, —á—Ç–æ –¥—É–º–∞–µ—Ç–µ –¥—Ä—É–∑—å—è?\"\"\"\n",
    "\n",
    "passage_2 = \"\"\"\n",
    "üè¶ –¢–∏–Ω—å–∫–æ—Ñ—Ñ\n",
    "\n",
    "–¢–∏–Ω—å–∫–æ—Ñ—Ñ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç—Å—è —Ä—ã–Ω–∫–æ–º –ø—Ä–∏–º–µ—Ä–Ω–æ –≤ 0,62 —Ç—Ä–ª–Ω —Ä—É–±., –ø—Ä–∏ —ç—Ç–æ–º –ø—Ä–∏–±—ã–ª—å –ø–æ –∏—Ç–æ–≥–∞–º 2023 –≥–æ–¥–∞ –º–æ–∂–µ—Ç —Å–æ—Å—Ç–∞–≤–∏—Ç—å –±–æ–ª–µ–µ 75 –º–ª—Ä–¥ —Ä—É–±.\n",
    "\n",
    "–û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, –¥–ª—è –∫–æ–º–ø–∞–Ω–∏–∏ —Å—Ç–æ–∏–º–æ—Å—Ç–∏ —Ç–∞–∫–∞—è –æ—Ü–µ–Ω–∫–∞ –±—ã–ª–∞ –±—ã —Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ–π, –Ω–æ –¢–∏–Ω—å–∫–æ—Ñ—Ñ ‚Äî —ç—Ç–æ –≤—Å–µ-—Ç–∞–∫–∏ –∫–æ–º–ø–∞–Ω–∏—è —Ä–æ—Å—Ç–∞, –∫–æ—Ç–æ—Ä–∞—è –ø–æ—Å–ª–µ–¥–Ω–∏–µ 5 –ª–µ—Ç –Ω–∞—Ä–∞—â–∏–≤–∞–ª–∞ –ø—Ä–∏–±—ã–ª—å –≤ —Å—Ä–µ–¥–Ω–µ–º –Ω–∞ 30% (–Ω–µ —Å—á–∏—Ç–∞—è 2022 –≥–æ–¥), –∫—Ä–æ–º–µ —Ç–æ–≥–æ, —Ç–µ–º–ø—ã —Ä–æ—Å—Ç–∞ –≤ —Å–ª–µ–¥—É—é—â–∏–µ 3 –≥–æ–¥–∞ –ø–æ —Å–∞–º—ã–º —Å–∫—Ä–æ–º–Ω—ã–º –æ–∂–∏–¥–∞–Ω–∏—è–º –±—É–¥—É—Ç –±–æ–ª–µ–µ 10% –≥–æ–¥–æ–≤—ã—Ö. \n",
    "\n",
    "–í–º–µ—Å—Ç–µ —Å —Ç–µ–º, –∫–æ–º–ø–∞–Ω–∏—è –Ω–µ—Å–µ—Ç –≤ —Å–µ–±–µ –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–µ —Ä–∏—Å–∫–∏, –∞ —Ç–∞–∫–∂–µ –¥–∞–∂–µ –µ—Å–ª–∏ –¢–∏–Ω—å–∫–æ—Ñ—Ñ –≤–æ–∑–æ–±–Ω–æ–≤–∏—Ç –¥–∏–≤–∏–¥–µ–Ω–¥–Ω—ã–µ –≤—ã–ø–ª–∞—Ç—ã, —Ç–æ –æ–Ω–∏ –±—É–¥—É—Ç –Ω–µ –±–æ–ª–µ–µ 5% –≥–æ–¥–æ–≤—ã—Ö. \n",
    "\n",
    "–ù–∞ —Å–∞–º–æ–º –¥–µ–ª–µ —Å–ª–æ–∂–Ω–æ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å—Å—è –º–µ–∂–¥—É –∞–∫—Ü–∏—è–º–∏ –°–±–µ—Ä–∞ –∏ –¢–∏–Ω—å–∫–æ—Ñ—Ñ, –Ω–æ –º–æ–∂–Ω–æ —Å –≤—ã—Å–æ–∫–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é —É—Ç–≤–µ—Ä–∂–¥–∞—Ç—å, —á—Ç–æ –¥–ª—è —Ç–æ–≥–æ, —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å –≤—ã—à–µ, —á–µ–º —Å–µ–π—á–∞—Å –≤ –æ–±–ª–∏–≥–∞—Ü–∏—è—Ö, –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –±—Ä–∞—Ç—å –Ω–∞ —Å–µ–±—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–∏—Å–∫–∏, –∏ —Ç—É—Ç —Ä–∏—Å–∫–æ–≤—ã–º –∏–Ω–≤–µ—Å—Ç–æ—Ä–∞–º –º–æ–≥—É—Ç –ø–æ–¥–æ–π—Ç–∏ –∏–º–µ–Ω–Ω–æ –∞–∫—Ü–∏–∏ TCS –∏–∑-–∑–∞ –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã—Ö —Ä–∏—Å–∫–æ–≤.\n",
    "\n",
    "–ß—Ç–æ –¥—É–º–∞–µ—Ç–µ –¥—Ä—É–∑—å—è, –∫—Ç–æ –ª—É—á—à–µ?\n",
    "\"\"\"\n",
    "\n",
    "tickers = ['TCS –¢–∏–Ω—å–∫–æ—Ñ—Ñ', 'SBER –°–±–µ—Ä–±–∞–Ω–∫', \"NVTK –ù–æ–≤–∞—Ç—ç–∫\"]\n",
    "\n",
    "input_texts = [passage_1] + tickers\n",
    "\n",
    "print(scores_calc(input_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['üè¶ –°–±–µ—Ä\\n\\n–ö–∞–∫–∏–µ —É –º–µ–Ω—è –º—ã—Å–ª–∏ –ø–æ –∫–æ–º–ø–∞–Ω–∏–∏.\\n\\n–≠—Ç–∞ –∫–æ–º–ø–∞–Ω–∏—è —Å–∞–º–∞—è –¥–æ—Ä–æ–≥–∞—è –≤ –†–æ—Å—Å–∏–∏, –µ–µ —Å—Ç–æ–∏–º–æ—Å—Ç—å –±–æ–ª–µ–µ 6 —Ç—Ä–ª–Ω —Ä—É–±. –ö–æ–Ω–µ—á–Ω–æ –∂–µ –¥–ª—è —ç—Ç–æ–≥–æ –µ—Å—Ç—å –æ—Å–Ω–æ–≤–∞–Ω–∏–µ ‚Äî –æ–Ω–∞ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–∞–º—É—é –±–æ–ª—å—à—É—é –ø—Ä–∏–±—ã–ª—å, –ø–æ –∫—Ä–∞–π–Ω–µ–π –º–µ—Ä–µ –≤ —ç—Ç–æ–º –≥–æ–¥—É –ø—Ä–∏–±—ã–ª—å –º–æ–∂–µ—Ç —Å–æ—Å—Ç–∞–≤–∏—Ç—å –±–æ–ª–µ–µ 1,5 —Ç—Ä–ª–Ω —Ä—É–±., –ø–æ—ç—Ç–æ–º—É –°–±–µ—Ä –ø–æ 6 —Ç—Ä–ª–Ω —É–∂–µ –∏ –Ω–µ –≤—ã–≥–ª—è–¥–∏—Ç —Ç–∞–∫ –¥–æ—Ä–æ–≥–æ. \\n\\n–ö–æ–º–ø–∞–Ω–∏—è –ø–ª–∞—Ç–∏—Ç –¥–æ—Å—Ç–æ–π–Ω—ã–µ –¥–∏–≤–∏–¥–µ–Ω–¥—ã, –∞ —Ç–∞–∫ –∫–∞–∫ –Ω–∞ —Ä—ã–Ω–∫–µ —Å–µ–π—á–∞—Å \"–ø—Ä–∞–≤—è—Ç\" —Ñ–∏–∑–∏–∫–∏, –¥–∏–≤–∏–¥–µ–Ω–¥—ã –≤ —Ç–∞–∫–∏—Ö –∫–æ–º–ø–∞–Ω–∏—è—Ö –±—É–¥–µ—Ç –æ—Å–Ω–æ–≤–æ–ø–æ–ª–∞–≥–∞—é—â–∏–º –≤ —Ä—ã–Ω–æ—á–Ω–æ–π –æ—Ü–µ–Ω–∫–µ. –ù–∞ –¥–∞–Ω–Ω—ã–π –º–æ–º–µ–Ω—Ç —è –æ–∂–∏–¥–∞—é, —á—Ç–æ –°–±–µ—Ä –∑–∞–ø–ª–∞—Ç–∏—Ç –±–æ–ª–µ–µ 33 —Ä—É–±. –¥–∏–≤–∏–¥–µ–Ω–¥–∞–º–∏, –ø—Ä–∏–º–µ—Ä–Ω–æ 34-35 —Ä—É–±. –°–±–µ—Ä –±—É–¥–µ—Ç –ø–ª–∞—Ç–∏—Ç—å –æ–∫–æ–ª–æ 12-14% –¥–∏–≤–∏–¥–µ–Ω–¥–∞–º–∏, —Å–µ–π—á–∞—Å —ç—Ç–æ –Ω–µ –∫–∞–∂–µ—Ç—Å—è –∫–∞–∫–æ–π-—Ç–æ –±–æ–ª—å—à–æ–π –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å—é –ø—Ä–∏ —Å—Ç–∞–≤–∫–µ 16% –∏ –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏ –±–µ–∑—Ä–∏—Å–∫–∞ –≤ –û–§–ó-–ü–ö –æ–∫–æ–ª–æ 16%, –∏ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã—Ö –û–§–ó-–ü–î –æ–∫–æ–ª–æ 12%, –Ω–æ –∫–æ–≥–¥–∞ —Å—Ç–∞–≤–∫—É —Å–Ω–∏–∑—è—Ç –¥–æ 8-9%, —Ç–∞–∫–∞—è –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å –±—É–¥–µ—Ç –≤—ã–≥–ª—è–¥–µ—Ç—å –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω–æ. –¢–µ–º –Ω–µ –º–µ–Ω–µ–µ –Ω–∞–¥–æ –ø–æ–Ω–∏–º–∞—Ç—å, —á—Ç–æ –∏ –û–§–ó-–ü–î –ø—Ä–∏ —Å–Ω–∏–∂–µ–Ω–∏–∏ —Å—Ç–∞–≤–∫–∏ –º–æ–≥—É—Ç –≤—ã—Ä–∞—Å—Ç–∏ –∏ —Å–æ—Å—Ç–∞–≤–∏—Ç—å –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏—é –°–±–µ—Ä—É –∫–∞–∫ –º–∏–Ω–∏–º—É–º –≤ 2024-2025 –≥–≥. \\n\\n–ö–æ–º–ø–∞–Ω–∏—é –æ–¥–Ω–æ–∑–Ω–∞—á–Ω–æ —Å—Ç–æ–∏—Ç —Ä–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å –ø–æ–¥—Ä–æ–±–Ω–æ –≤ 2024 –≥–æ–¥—É, —á—Ç–æ –¥—É–º–∞–µ—Ç–µ –¥—Ä—É–∑—å—è?',\n",
       " 'TCS –¢–∏–Ω—å–∫–æ—Ñ—Ñ',\n",
       " 'SBER –°–±–µ—Ä–±–∞–Ω–∫',\n",
       " 'NVTK –ù–æ–≤–∞—Ç—ç–∫']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "moex_data = pd.read_csv('moex_tickers.csv', on_bad_lines='skip', encoding='windows-1251')\n",
    "moex_data = moex_data[~moex_data['TRADE_CODE'].isnull()]\n",
    "moex_data = moex_data[['TRADE_CODE', 'EMITENT_FULL_NAME', 'INSTRUMENT_TYPE']]\n",
    "moex_data = moex_data[moex_data['INSTRUMENT_TYPE'] == '–ê–∫—Ü–∏—è –æ–±—ã–∫–Ω–æ–≤–µ–Ω–Ω–∞—è']\n",
    "moex_data = moex_data[['TRADE_CODE', 'EMITENT_FULL_NAME']]\n",
    "moex_data['ticker_and_name'] = moex_data['TRADE_CODE'] + ' ' + moex_data['EMITENT_FULL_NAME']\n",
    "moex_tickers = moex_data['ticker_and_name'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ticker(messages, moex_tickers):\n",
    "    input_texts = messages + moex_tickers\n",
    "    pred_labels_matrix = scores_calc(input_texts, len(messages))\n",
    "    pred_labels = pred_labels_matrix.argmax(axis=1)\n",
    "    pred_tickers = [moex_tickers[l] for l in pred_labels]\n",
    "    return pred_tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MAGN –ü—É–±–ª–∏—á–Ω–æ–µ –∞–∫—Ü–∏–æ–Ω–µ—Ä–Ω–æ–µ –æ–±—â–µ—Å—Ç–≤–æ \"–ú–∞–≥–Ω–∏—Ç–æ–≥–æ—Ä—Å–∫–∏–π –º–µ—Ç–∞–ª–ª—É—Ä–≥–∏—á–µ—Å–∫–∏–π –∫–æ–º–±–∏–Ω–∞—Ç\"',\n",
       " 'MTLR –ü—É–±–ª–∏—á–Ω–æ–µ –∞–∫—Ü–∏–æ–Ω–µ—Ä–Ω–æ–µ –æ–±—â–µ—Å—Ç–≤–æ \"–ú–µ—á–µ–ª\"',\n",
       " 'MTSS –ü—É–±–ª–∏—á–Ω–æ–µ –∞–∫—Ü–∏–æ–Ω–µ—Ä–Ω–æ–µ –æ–±—â–µ—Å—Ç–≤–æ \"–ú–æ–±–∏–ª—å–Ω—ã–µ –¢–µ–ª–µ–°–∏—Å—Ç–µ–º—ã\"',\n",
       " 'MOEX –ü—É–±–ª–∏—á–Ω–æ–µ –∞–∫—Ü–∏–æ–Ω–µ—Ä–Ω–æ–µ –æ–±—â–µ—Å—Ç–≤–æ \"–ú–æ—Å–∫–æ–≤—Å–∫–∞—è –ë–∏—Ä–∂–∞ –ú–ú–í–ë-–†–¢–°\"',\n",
       " 'LKOH –ü—É–±–ª–∏—á–Ω–æ–µ –∞–∫—Ü–∏–æ–Ω–µ—Ä–Ω–æ–µ –æ–±—â–µ—Å—Ç–≤–æ \"–ù–µ—Ñ—Ç—è–Ω–∞—è –∫–æ–º–ø–∞–Ω–∏—è \"–õ–£–ö–û–ô–õ\"',\n",
       " 'BELU –ü—É–±–ª–∏—á–Ω–æ–µ –∞–∫—Ü–∏–æ–Ω–µ—Ä–Ω–æ–µ –æ–±—â–µ—Å—Ç–≤–æ \"–ù–æ–≤–∞–ë–µ–≤ –ì—Ä—É–ø–ø\"',\n",
       " 'NLMK –ü—É–±–ª–∏—á–Ω–æ–µ –∞–∫—Ü–∏–æ–Ω–µ—Ä–Ω–æ–µ –æ–±—â–µ—Å—Ç–≤–æ \"–ù–æ–≤–æ–ª–∏–ø–µ—Ü–∫–∏–π –º–µ—Ç–∞–ª–ª—É—Ä–≥–∏—á–µ—Å–∫–∏–π –∫–æ–º–±–∏–Ω–∞—Ç\"',\n",
       " 'PIKK –ü—É–±–ª–∏—á–Ω–æ–µ –∞–∫—Ü–∏–æ–Ω–µ—Ä–Ω–æ–µ –æ–±—â–µ—Å—Ç–≤–æ \"–ü–ò–ö-—Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∑–∞—Å—Ç—Ä–æ–π—â–∏–∫\"',\n",
       " 'PLZL –ü—É–±–ª–∏—á–Ω–æ–µ –∞–∫—Ü–∏–æ–Ω–µ—Ä–Ω–æ–µ –æ–±—â–µ—Å—Ç–≤–æ \"–ü–æ–ª—é—Å\"',\n",
       " 'RTKM –ü—É–±–ª–∏—á–Ω–æ–µ –∞–∫—Ü–∏–æ–Ω–µ—Ä–Ω–æ–µ –æ–±—â–µ—Å—Ç–≤–æ \"–†–æ—Å—Ç–µ–ª–µ–∫–æ–º\"',\n",
       " 'SBER –ü—É–±–ª–∏—á–Ω–æ–µ –∞–∫—Ü–∏–æ–Ω–µ—Ä–Ω–æ–µ –æ–±—â–µ—Å—Ç–≤–æ \"–°–±–µ—Ä–±–∞–Ω–∫ –†–æ—Å—Å–∏–∏\"',\n",
       " 'CHMF –ü—É–±–ª–∏—á–Ω–æ–µ –∞–∫—Ü–∏–æ–Ω–µ—Ä–Ω–æ–µ –æ–±—â–µ—Å—Ç–≤–æ \"–°–µ–≤–µ—Ä—Å—Ç–∞–ª—å\"',\n",
       " 'SELG –ü—É–±–ª–∏—á–Ω–æ–µ –∞–∫—Ü–∏–æ–Ω–µ—Ä–Ω–æ–µ –æ–±—â–µ—Å—Ç–≤–æ \"–°–µ–ª–∏–≥–¥–∞—Ä\"',\n",
       " 'SVCB –ü—É–±–ª–∏—á–Ω–æ–µ –∞–∫—Ü–∏–æ–Ω–µ—Ä–Ω–æ–µ –æ–±—â–µ—Å—Ç–≤–æ \"–°–æ–≤–∫–æ–º–±–∞–Ω–∫\"',\n",
       " 'FLOT –ü—É–±–ª–∏—á–Ω–æ–µ –∞–∫—Ü–∏–æ–Ω–µ—Ä–Ω–æ–µ –æ–±—â–µ—Å—Ç–≤–æ \"–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏–π —Ñ–ª–æ—Ç\"',\n",
       " 'TGKA –ü—É–±–ª–∏—á–Ω–æ–µ –∞–∫—Ü–∏–æ–Ω–µ—Ä–Ω–æ–µ –æ–±—â–µ—Å—Ç–≤–æ \"–¢–µ—Ä—Ä–∏—Ç–æ—Ä–∏–∞–ª—å–Ω–∞—è –≥–µ–Ω–µ—Ä–∏—Ä—É—é—â–∞—è –∫–æ–º–ø–∞–Ω–∏—è ‚Ññ1\"',\n",
       " 'HYDR –ü—É–±–ª–∏—á–Ω–æ–µ –∞–∫—Ü–∏–æ–Ω–µ—Ä–Ω–æ–µ –æ–±—â–µ—Å—Ç–≤–æ \"–§–µ–¥–µ—Ä–∞–ª—å–Ω–∞—è –≥–∏–¥—Ä–æ–≥–µ–Ω–µ—Ä–∏—Ä—É—é—â–∞—è –∫–æ–º–ø–∞–Ω–∏—è - –†—É—Å–ì–∏–¥—Ä–æ\"',\n",
       " 'FEES –ü—É–±–ª–∏—á–Ω–æ–µ –∞–∫—Ü–∏–æ–Ω–µ—Ä–Ω–æ–µ –æ–±—â–µ—Å—Ç–≤–æ \"–§–µ–¥–µ—Ä–∞–ª—å–Ω–∞—è —Å–µ—Ç–µ–≤–∞—è –∫–æ–º–ø–∞–Ω–∏—è - –†–æ—Å—Å–µ—Ç–∏\"',\n",
       " 'PHOR –ü—É–±–ª–∏—á–Ω–æ–µ –∞–∫—Ü–∏–æ–Ω–µ—Ä–Ω–æ–µ –æ–±—â–µ—Å—Ç–≤–æ \"–§–æ—Å–ê–≥—Ä–æ\"',\n",
       " 'ELFV –ü—É–±–ª–∏—á–Ω–æ–µ –∞–∫—Ü–∏–æ–Ω–µ—Ä–Ω–æ–µ –æ–±—â–µ—Å—Ç–≤–æ \"–≠–õ5-–≠–Ω–µ—Ä–≥–æ\"']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moex_tickers[20:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 67998720 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-c6373254d774>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpredict_ticker\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpassage_2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmoex_tickers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-18-e118796d952e>\u001b[0m in \u001b[0;36mpredict_ticker\u001b[1;34m(messages, moex_tickers)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpredict_ticker\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmoex_tickers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0minput_texts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmessages\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmoex_tickers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mpred_labels_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscores_calc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_texts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mpred_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpred_labels_matrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mpred_tickers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mmoex_tickers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpred_labels\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-ee6c6371b06f>\u001b[0m in \u001b[0;36mscores_calc\u001b[1;34m(input_texts, len_messages)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mbatch_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_texts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'pt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mbatch_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0membeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maverage_pool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'attention_mask'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1011\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1012\u001b[0m         )\n\u001b[1;32m-> 1013\u001b[1;33m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[0;32m   1014\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1015\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    605\u001b[0m                 )\n\u001b[0;32m    606\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 607\u001b[1;33m                 layer_outputs = layer_module(\n\u001b[0m\u001b[0;32m    608\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    495\u001b[0m         \u001b[1;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    496\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 497\u001b[1;33m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[0;32m    498\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    499\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    425\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m     ) -> Tuple[torch.Tensor]:\n\u001b[1;32m--> 427\u001b[1;33m         self_outputs = self.self(\n\u001b[0m\u001b[0;32m    428\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    365\u001b[0m         \u001b[0mcontext_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattention_probs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue_layer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 367\u001b[1;33m         \u001b[0mcontext_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontext_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    368\u001b[0m         \u001b[0mnew_context_layer_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontext_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall_head_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    369\u001b[0m         \u001b[0mcontext_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontext_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_context_layer_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 67998720 bytes."
     ]
    }
   ],
   "source": [
    "predict_ticker([passage_2], moex_tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "translator = GoogleTranslator(source='auto', target='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('result.json', 'r', encoding='utf-8') as f:\n",
    "    tg_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_messages = tg_data['messages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\u200b\\u200b',\n",
       " '–ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ',\n",
       " '',\n",
       " '–ê–∫—Ü–∏–∏ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –∫–æ–º–ø–∞–Ω–∏–π. ',\n",
       " '–≠—Ç–æ –æ–¥–∏–Ω –∏–∑ —Å–∞–º—ã—Ö —Ä–∏—Å–∫–æ–≤—ã—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ —Ä—ã–Ω–∫–µ –≤ —Ä—É–±–ª—è—Ö, –Ω–æ –ø–æ—á–µ–º—É-—Ç–æ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ 100% –∏–Ω–≤–µ—Å—Ç–æ—Ä–æ–≤ –∏—Å–ø–æ–ª—å–∑—É—é—Ç —ç—Ç–æ—Ç –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞, –¥–æ–ø—É—Å—Ç–∏–º—ã—Ö —Ä–∏—Å–∫–æ–≤, —Ü–µ–ª–∏ –∏–Ω–≤–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏ —Ç.–¥. –ö–æ–º—É –ø–æ–¥–æ–π–¥–µ—Ç –ø–æ–∫—É–ø–∫–∞ –∞–∫—Ü–∏–π? –Ø –±—ã –Ω–µ —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–ª–∞ –ø–æ–∫—É–ø–∞—Ç—å –∏—Ö —Å –≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–º –¥–æ 3 –∏–ª–∏ –¥–∞–∂–µ 5 –ª–µ—Ç. –ù–∞—à–∞ —Å—Ç—Ä–∞–Ω–∞ –≤ –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω–æ–º –º–∏—Ä–µ —Å–µ–π—á–∞—Å –æ–±–ª–∞–¥–∞–µ—Ç —Å–∞–º—ã–º–∏ –±–æ–ª—å—à–∏–º–∏ —Ä–∏—Å–∫–∞–º–∏, –∏ –ª—é–±–æ–π –≥–ª–æ–±–∞–ª—å–Ω—ã–π –∏–Ω–≤–µ—Å—Ç–æ—Ä —Å–∫–∞–∂–µ—Ç, —á—Ç–æ –±–æ–ª—å—à–µ –≤—Å–µ–≥–æ —Ä–∏—Å–∫–æ–≤ –Ω–∞ —Ñ–æ–Ω–¥–æ–≤–æ–º —Ä—ã–Ω–∫–µ –∞–∫—Ü–∏–π —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –∫–æ–º–ø–∞–Ω–∏–π. –ü—Ä–µ–∂–¥–µ —á–µ–º –∏–Ω–≤–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –≤ –∞–∫—Ü–∏–∏, —ç—Ç–æ –Ω—É–∂–Ω–æ –æ—Å–æ–∑–Ω–∞—Ç—å. –°–∞–º—ã–µ-—Å–∞–º—ã–µ –±–æ–ª—å—à–∏–µ —Ä–∏—Å–∫–∏ –∏–º–µ–Ω–Ω–æ –≤ –∞–∫—Ü–∏—è—Ö —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –∫–æ–º–ø–∞–Ω–∏–π. –ù–µ —Å—Ç–æ–∏—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–∞–∫–æ–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç, –µ—Å–ª–∏ –≤—ã –∫–æ–ø–∏—Ç–µ –Ω–∞ –º–∞—à–∏–Ω—É, –∫–≤–∞—Ä—Ç–∏—Ä—É, —É—á–µ–±—É, –ª–µ—á–µ–Ω–∏–µ, –æ—Ç–¥—ã—Ö –∏ —Ç.–¥. –¢–∞–∫–∂–µ –Ω–µ —Å—Ç–æ–∏—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —ç—Ç–æ—Ç –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç —Å –≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–º –¥–æ 3-5 –ª–µ—Ç (–±—É–¥–µ—Ç –∑–∞–≤–∏—Å–µ—Ç—å –æ—Ç –Ω–∞–¥—ë–∂–Ω–æ—Å—Ç–∏ –∫–æ–º–ø–∞–Ω–∏–π). –ï—Å–ª–∏ –≤—ã —Ä–µ—à–∏–ª–∏ –≤—Å—ë-—Ç–∞–∫–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç, —Ç–æ –ø–æ–π–º–∏—Ç–µ, —á—Ç–æ –ø–æ–∫—É–ø–∫–∞ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –∫–æ–º–ø–∞–Ω–∏–π –º–æ–∂–µ—Ç –±—ã—Ç—å —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ —Ç–æ–≥–æ, –∫–∞–∫ –≤—ã —Ä–∞–∑–±–µ—Ä—ë—Ç–µ –≤—Å–µ —Ä–∏—Å–∫–∏ —ç–º–∏—Ç–µ–Ω—Ç–∞ –∏ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç–µ –≤—Å–µ —Ç—Ä–∏–≥–≥–µ—Ä—ã —Ä–æ—Å—Ç–∞. –ü–æ—á–µ–º—É —Ç–∞–∫? –ü–æ—Ç–æ–º—É —á—Ç–æ –≤ —Å–ª—É—á–∞–µ –ø–∞–¥–µ–Ω–∏—è —Ä—ã–Ω–æ—á–Ω–æ–π —Å—Ç–æ–∏–º–æ—Å—Ç–∏, –≤—ã –ø—Ä–æ—Å—Ç–æ –Ω–µ —Å–º–æ–∂–µ—Ç–µ –ø–æ–Ω—è—Ç—å, –∞ –¥–ª—è —á–µ–≥–æ –≤—ã –ø–æ–∫—É–ø–∞–ª–∏ –∫–æ–º–ø–∞–Ω–∏—é, –ø–æ—á–µ–º—É –æ–Ω–∞ –±—É–¥–µ—Ç —Å—Ç–æ–∏—Ç—å –¥–æ—Ä–æ–∂–µ –≤ –±—É–¥—É—â–µ–º –∏ –∫–∞–∫–∏–µ —Ä–µ–∞–ª—å–Ω—ã–µ —Ä–∏—Å–∫–∏ –º–æ–≥—É—Ç —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å—Å—è –∏–ª–∏ —Ä–µ–∞–ª–∏–∑—É—é—Ç—Å—è. –ï—Å–ª–∏ –≤—ã —ç—Ç–æ –Ω–µ –≤—ã–ø–∏—Å–∞–ª–∏ —Å–µ–±–µ –≤ –±–ª–æ–∫–Ω–æ—Ç, —Ç–µ–ª–µ—Ñ–æ–Ω –∏ —Ç.–¥, —Ç–æ –≤—ã –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ–¥–∞–¥–∏—Ç–µ –ø–∞–¥–∞—é—â—É—é –∞–∫—Ü–∏—é –∏ –∫—É–ø–∏—Ç–µ –¥—Ä—É–≥—É—é, –ø–æ—Ç–æ–º –ø—Ä–æ–¥–∞–≤–∏—Ç–µ —Ç—É, —á—Ç–æ –∫—É–ø–∏–ª–∏, –µ—Å–ª–∏ –æ–Ω–∞ —Ç–æ–∂–µ –±—É–¥–µ—Ç –ø–∞–¥–∞—Ç—å –∏ –∫—É–ø–∏—Ç—å –µ—â—ë –æ–¥–Ω—É, –≤ –∏—Ç–æ–≥–µ –ø–æ—Ç–µ—Ä—è–µ—Ç–µ –¥–µ–Ω—å–≥–∏, –∏ —Ç–æ–ª—å–∫–æ –≤ —Å–ª—É—á–∞–µ, –µ—Å–ª–∏ —Å—Ä–∞–∑—É –ø–æ—Å–ª–µ –≤–∞—à–µ–π –ø–æ–∫—É–ø–∫–∏ –∞–∫—Ü–∏—è –Ω–∞—á–Ω—ë—Ç —Ä–∞—Å—Ç–∏, –≤—ã —Å–º–æ–∂–µ—Ç–µ —Å–ø–æ–∫–æ–π–Ω–æ –±—ã—Ç—å –∏–Ω–≤–µ—Å—Ç–æ—Ä–æ–º. –ò—Ç–∞–∫ –æ—Ç–¥–µ–ª—å–Ω—ã–µ –∫–æ–º–ø–∞–Ω–∏–∏ –†–æ—Å—Å–∏–∏ —Ç–æ–ª—å–∫–æ –Ω–∞ 3 –≥–æ–¥–∞ –∏ –±–æ–ª–µ–µ –∏ —ç—Ç–æ—Ç –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç —Å —Å–∞–º—ã–º-—Å–∞–º—ã–º –±–æ–ª—å—à–∏–º —Ä–∏—Å–∫–æ–º, –Ω–æ –∏ —Å —Å–∞–º—ã–º –±–æ–ª—å—à–∏–º –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–æ–º, —Ö–æ—Ç—è —Å–µ–π—á–∞—Å —Ä—ã–Ω–æ–∫ –∞–∫—Ü–∏–π –Ω–∞ –º–æ–π –≤–∑–≥–ª—è–¥ –≤—ã–≥–ª—è–¥–∏—Ç –º–µ–Ω–µ–µ –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω–æ, —á–µ–º –æ–±–ª–∏–≥–∞—Ü–∏–∏, —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ñ–æ–Ω–¥—ã, –∑–æ–ª–æ—Ç–æ –∏ –¥–∞–∂–µ –¥–µ–ø–æ–∑–∏—Ç.',\n",
       " '–ö—É–¥–∞ –≤ –∏—Ç–æ–≥–µ –∏–Ω–≤–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å? ',\n",
       " '–ï—Å–ª–∏ –≤—ã –ø–ª–∞–Ω–∏—Ä—É–µ—Ç–µ —á—Ç–æ-—Ç–æ –ø—Ä–∏–æ–±—Ä–µ—Å—Ç–∏ –≤ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–µ',\n",
       " '–º–µ—Å—è—Ü–µ–≤',\n",
       " ', —Ç–æ –ø—Ä–æ—Å—Ç–æ –ø—É—Å—Ç—å —Ä—É–±–ª–∏',\n",
       " '–ª–µ–∂–∞—Ç –Ω–∞ –∫–∞—Ä—Ç–µ',\n",
       " '–∏ –∫–∞–ø–∞—é—Ç –ø—Ä–æ—Ü–µ–Ω—Ç—ã. \\n\\n–ï—Å–ª–∏ –∫–æ–ø–∏—Ç–µ –Ω–∞',\n",
       " '—É—á–µ–±—É, –∂–∏–ª—å—ë, –º–∞—à–∏–Ω—É, –ª–µ—á–µ–Ω–∏–µ ',\n",
       " '–∏ —ç—Ç–∞ –ø–æ–∫—É–ø–∫–∞ –Ω–∞—Å—Ç—É–ø–∏—Ç —á–µ—Ä–µ–∑ 1-3 –≥–æ–¥–∞, —Ç–æ —Å–∞–º—ã–π –±–µ–∑–æ–ø–∞—Å–Ω—ã–π —Å–ø–æ—Å–æ–± ‚Äî',\n",
       " '–¥–µ–ø–æ–∑–∏—Ç',\n",
       " '(—Ç–∞–º –µ—Å—Ç—å —Å—Ç—Ä–∞—Ö–æ–≤–∫–∞). \\n\\n–ï—Å–ª–∏ —É –≤–∞—Å –µ—Å—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω—ã–π –≥–æ—Ä–∏–∑–æ–Ω—Ç –∏ –≤–∞—à–∞ —Ü–µ–ª—å —É–∫–ª–∞–¥—ã–≤–∞–µ—Ç—Å—è –≤ –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å',\n",
       " '–û–§–ó',\n",
       " ', —Ç–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø—Ä–æ—Å—Ç–æ –∫—É–ø–∏—Ç—å —ç—Ç–∏ –û–§–ó, –Ω–µ –Ω–∞–¥–æ –ø—Ä–∏–¥—É–º—ã–≤–∞—Ç—å –≤–µ–ª–æ—Å–∏–ø–µ–¥. –ï—Å–ª–∏ —á–µ—Ä–µ–∑ –ò–ò–°, —Ç–æ –æ—Ç 3-—Ö –ª–µ—Ç. –ï—Å–ª–∏ –Ω–µ —Ö–æ—Ç–∏—Ç–µ –ø–∞—Ä–∏—Ç—å—Å—è –∏ —Ä–µ–∏–Ω–≤–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –∫—É–ø–æ–Ω—ã, —Ç–æ –±–µ—Ä–∏—Ç–µ —Ñ–æ–Ω–¥ –æ–±–ª–∏–≥–∞—Ü–∏–π. \\n\\n–ï—Å–ª–∏ –≤—ã –æ–±–ª–∞–¥–∞–µ—Ç–µ –æ–≥—Ä–æ–º–Ω–æ–π —Å—É–º–º–æ–π –∏ –±–µ—Å—Å—Ä–æ—á–Ω—ã–º –≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–º, —Ç–æ –º–æ–∂–Ω–æ —á–∞—Å—Ç—å —Å—Ä–µ–¥—Å—Ç–≤ –ø—É—Å—Ç–∏—Ç—å –Ω–∞ –ø–æ–∫—É–ø–∫—É',\n",
       " '–Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏',\n",
       " ', –µ—Å–ª–∏ –Ω–µ–±–æ–ª—å—à–æ–π –ø–æ—Ä—Ç—Ñ–µ–ª—å, —Ç–æ –Ω–∞ –ø–æ–∫—É–ø–∫—É',\n",
       " '–ó–ü–ò–§',\n",
       " '. \\n\\n–î–ª—è –∑–∞—â–∏—Ç—ã –æ—Ç –¥–µ–≤–∞–ª—å–≤–∞—Ü–∏–∏ —Ä—É–±–ª—è –∏ —Ä–æ—Å—Ç–∞ –∏–Ω—Ñ–ª—è—Ü–∏–æ–Ω–Ω—ã—Ö –æ–∂–∏–¥–∞–Ω–∏–π –≤ –º–∏—Ä–µ —Å—Ç–æ–∏—Ç –ø–æ–∫—É–ø–∞—Ç—å',\n",
       " '–∑–æ–ª–æ—Ç–æ',\n",
       " ', –Ω–æ –Ω–µ –Ω–∞ –≤–µ—Å—å –ø–æ—Ä—Ç—Ñ–µ–ª—å, –∞ –¥–æ 25%.\\n\\n–ï—Å–ª–∏ –≤—ã –∏—â–∏—Ç–µ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –∏ —Ö–æ—Ä–æ—à—É—é —É–ø—Ä–∞–≤–ª—è—é—â—É—é –∫–æ–º–ø–∞–Ω–∏—é –Ω–∞ 3 –≥–æ–¥–∞ –∏ –±–æ–ª–µ–µ, —Ç–æ —Å—Ç–æ–∏—Ç —Ä–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å',\n",
       " '—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ñ–æ–Ω–¥—ã',\n",
       " '(–∞–∫—Ü–∏–∏, –æ–±–ª–∏–≥–∞—Ü–∏–∏, –∑–æ–ª–æ—Ç–æ). \\n\\n–ï—Å–ª–∏ —É –≤–∞—Å –æ—á–µ–Ω—å –±–æ–ª—å—à–æ–π –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω—ã–π –≥–æ—Ä–∏–∑–æ–Ω—Ç –∏ –Ω–µ—Ç –≤—Ä–µ–º–µ–Ω–∏, —Ç–æ –º–æ–∂–Ω–æ —Ä–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å',\n",
       " '—Ñ–æ–Ω–¥ –∞–∫—Ü–∏–π',\n",
       " ', –µ—Å–ª–∏ –µ—Å—Ç—å –≤—Ä–µ–º—è —Ä–∞–∑–±–∏—Ä–∞—Ç—å—Å—è –≤ —Ä–∏—Å–∫–∞—Ö –∏ —Ç—Ä–∏–≥–≥–µ—Ä–∞—Ö —Ä–æ—Å—Ç–∞, —Ç–æ –æ—Ç–¥–µ–ª—å–Ω—ã–µ –∫–æ–º–ø–∞–Ω–∏–∏.',\n",
       " '–ü–æ—á–µ–º—É –±—ã –Ω–µ –≤–∑—è—Ç—å –≤—Å–µ?',\n",
       " '–ï—Å–ª–∏ —É –≤–∞—Å –¥–ª–∏—Ç–µ–ª—å–Ω—ã–π —Å—Ä–æ–∫ –∏–Ω–≤–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –±–æ–ª—å—à–æ–π –ø–æ—Ä—Ç—Ñ–µ–ª—å, —Ç–æ –º–æ–∂–Ω–æ —Å–æ–±—Ä–∞—Ç—å –ø–æ—Ä—Ç—Ñ–µ–ª—å –∏–∑ –≤—Å–µ—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤.',\n",
       " '–ü—Ä–∏–º–µ—Ä –±—ã–ª –≤—ã—à–µ',\n",
       " '–†–∏—Å–∫–∏ –µ—Å—Ç—å –≤ –∫–∞–∂–¥–æ–º –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–µ, –¥–∞–∂–µ –Ω–∞ –≤–∫–ª–∞–¥–µ, –Ω–∞–ø—Ä–∏–º–µ—Ä, —Ä–∏—Å–∫ –¥–µ–≤–∞–ª—å–≤–∞—Ü–∏–∏ —Ä—É–±–ª—è –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤–µ–∑–¥–µ, –∫—Ä–æ–º–µ –∑–æ–ª–æ—Ç–∞ –∏ —á–∞—Å—Ç–∏—á–Ω–æ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏.',\n",
       " 'P.S. –ë–æ–Ω—É—Å–æ–º –¥–ª—è —Ç–µ—Ö, –∫—Ç–æ –¥–æ—á–∏—Ç–∞–ª, –Ω–∏–∂–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é –ø–æ—Ä—Ç—Ñ–µ–ª—å –Ω–∞ 2 –≥–æ–¥–∞, –∫–æ—Ç–æ—Ä—ã–π —è –±—ã —Å–æ–±–∏—Ä–∞–ª–∞ –≤ —Ä—É–±–ª—è—Ö.',\n",
       " '–°–∞–º–æ–µ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ–µ, —á—Ç–æ –ø–æ —Ç–∞–∫–æ–º—É –ø—Ä–æ—Å—Ç–æ–º—É –ø–æ—Ä—Ç—Ñ–µ–ª—é –º–æ–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å –æ–∫–æ–ª–æ 35% –∑–∞ 2 –≥–æ–¥–∞.']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[text_chunk.get('text', '') if isinstance(text_chunk, dict) else text_chunk.strip() for text_chunk in channel_messages[-109]['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 5014,\n",
       " 'type': 'message',\n",
       " 'date': '2023-11-12T17:52:34',\n",
       " 'date_unixtime': '1699800754',\n",
       " 'edited': '2023-11-12T17:52:48',\n",
       " 'edited_unixtime': '1699800768',\n",
       " 'from': 'NataliaBaffetovna | –ò–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏',\n",
       " 'from_id': 'channel1400938880',\n",
       " 'text': ['–ö–æ–≥–¥–∞ —è –±—É–¥—É –ø–æ–∫—É–ø–∞—Ç—å –æ–±–ª–∏–≥–∞—Ü–∏–∏, –ø–∏—Å–∞–ª–∞ —Ç—É—Ç: ',\n",
       "  {'type': 'link', 'text': 'https://t.me/NataliaBaff/4949'},\n",
       "  ''],\n",
       " 'text_entities': [{'type': 'plain',\n",
       "   'text': '–ö–æ–≥–¥–∞ —è –±—É–¥—É –ø–æ–∫—É–ø–∞—Ç—å –æ–±–ª–∏–≥–∞—Ü–∏–∏, –ø–∏—Å–∞–ª–∞ —Ç—É—Ç: '},\n",
       "  {'type': 'link', 'text': 'https://t.me/NataliaBaff/4949'},\n",
       "  {'type': 'plain', 'text': ''}]}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channel_messages[-102]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
